{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9447f7-638e-4b06-a63f-cbcb4f9fca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092a8e11-ecfa-4427-b4e5-e5e2625b52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "student_files = [doc for doc in os.listdir() if doc.endswith('.txt')]\n",
    "student_notes = [open(_file, encoding='utf-8').read() for _file in student_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f33dd8-731e-4833-9d47-6f25470fff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if documents are empty\n",
    "if not all(student_notes):\n",
    "    raise ValueError(\"One or more documents are empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c439a3-05ec-4170-a463-c369e4243cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorize function\n",
    "def vectorize(Text): \n",
    "    return TfidfVectorizer(stop_words=None).fit_transform(Text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737d21cd-b413-44a6-9a4b-d43abd736de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity function\n",
    "def similarity(doc1, doc2): \n",
    "    return cosine_similarity([doc1, doc2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f47b1b-dbd0-46dd-8413-858bffb0846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "vectors = vectorize(student_notes)\n",
    "s_vectors = list(zip(student_files, vectors))\n",
    "plagiarism_results = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf7ebad-6d0e-45ed-8af8-b5eacfd5a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for plagiarism\n",
    "def check_plagiarism():\n",
    "    global s_vectors\n",
    "    for student_a, text_vector_a in s_vectors:\n",
    "        new_vectors = s_vectors.copy()\n",
    "        current_index = new_vectors.index((student_a, text_vector_a))\n",
    "        del new_vectors[current_index]\n",
    "        for student_b, text_vector_b in new_vectors:\n",
    "            sim_score = similarity(text_vector_a, text_vector_b)[0][1]\n",
    "            student_pair = sorted((student_a, student_b))\n",
    "            score = (student_pair[0], student_pair[1], sim_score)\n",
    "            plagiarism_results.add(score)\n",
    "    return plagiarism_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6336407c-8202-4130-a376-504281c7af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fatma.txt', 'juma.txt', 0.20179089793739657)\n",
      "('fatma.txt', 'john.txt', 0.16228391831223246)\n",
      "('john.txt', 'juma.txt', 0.5713243251172899)\n",
      "('juma.txt', 'requirements.txt', 0.0)\n",
      "('john.txt', 'requirements.txt', 0.0)\n",
      "('fatma.txt', 'requirements.txt', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "for data in check_plagiarism():\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65581b5b-f776-422e-8ca3-c9b7671078b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "EPS  = np.finfo('float32').eps\n",
    "\n",
    "\n",
    "class TrafficLightClassifier:\n",
    "\n",
    "    def __init__(self, input_shape, learning_rate, verbose=True):\n",
    "\n",
    "        # Placeholders\n",
    "        input_h, input_w = input_shape\n",
    "        self.x = tf.placeholder(dtype=tf.float32, shape=[None, input_h, input_w, 3])  # input placeholder\n",
    "        self.targets = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32)  # dropout keep probability\n",
    "\n",
    "        self.n_classes      = 4              # {void, red, yellow, green}\n",
    "        self.learning_rate  = learning_rate  # learning rate used in train step\n",
    "\n",
    "        self._inference     = None\n",
    "        self._loss          = None\n",
    "        self._train_step    = None\n",
    "        self._accuracy      = None\n",
    "        self._summaries     = None\n",
    "\n",
    "        self.inference\n",
    "        self.loss\n",
    "        self.train_step\n",
    "        self.accuracy\n",
    "        # self.summaries # todo add these\n",
    "\n",
    "        if verbose:\n",
    "            self.print_summary()\n",
    "\n",
    "    @property\n",
    "    def inference(self):\n",
    "        if self._inference is None:\n",
    "            with tf.variable_scope('inference'):\n",
    "\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(1e-3)\n",
    "\n",
    "                conv1_filters = 32\n",
    "                conv1 = tf.layers.conv2d(self.x, conv1_filters, kernel_size=(3, 3), padding='same',\n",
    "                                         activation=tf.nn.relu, kernel_regularizer=kernel_regularizer)\n",
    "                pool1 = tf.layers.max_pooling2d(conv1, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "\n",
    "                conv2_filters = 64\n",
    "                conv2 = tf.layers.conv2d(pool1, conv2_filters, kernel_size=(3, 3), padding='same',\n",
    "                                         activation=tf.nn.relu, kernel_regularizer=kernel_regularizer)\n",
    "                pool2 = tf.layers.max_pooling2d(conv2, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "\n",
    "                _, h, w, c = pool2.get_shape().as_list()\n",
    "                pool2_flat = tf.reshape(pool2, shape=[-1, h * w * c])\n",
    "\n",
    "                pool2_drop = tf.nn.dropout(pool2_flat, keep_prob=self.keep_prob)\n",
    "\n",
    "                hidden_units = self.n_classes\n",
    "                hidden = tf.layers.dense(pool2_drop, units=hidden_units, activation=tf.nn.relu)\n",
    "\n",
    "                logits = tf.layers.dense(hidden, units=self.n_classes, activation=None)\n",
    "\n",
    "                self._inference = tf.nn.softmax(logits)\n",
    "\n",
    "        return self._inference\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        if self._loss is None:\n",
    "            with tf.variable_scope('loss'):\n",
    "                predictions = self.inference\n",
    "                targets_onehot = tf.one_hot(self.targets, depth=self.n_classes)\n",
    "                self._loss = tf.reduce_mean(-tf.reduce_sum(targets_onehot * tf.log(predictions + EPS), reduction_indices=1))\n",
    "        return self._loss\n",
    "\n",
    "    @property\n",
    "    def train_step(self):\n",
    "        if self._train_step is None:\n",
    "            with tf.variable_scope('training'):\n",
    "                self._train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        return self._train_step\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        if self._accuracy is None:\n",
    "            with tf.variable_scope('accuracy'):\n",
    "                correct_predictions = tf.equal(tf.argmax(self.inference, axis=1),\n",
    "                                               tf.argmax(tf.one_hot(self.targets, depth=self.n_classes), axis=1))\n",
    "                self._accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "        return self._accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def print_summary():\n",
    "        def pretty_border():\n",
    "            print('*' * 50)\n",
    "\n",
    "        pretty_border()\n",
    "        print('Classifier initialized.')\n",
    "\n",
    "        trainable_variables  = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        num_trainable_params = np.sum([np.prod(v.get_shape()) for v in trainable_variables])\n",
    "        print('Number of trainable parameters: {}'.format(num_trainable_params))\n",
    "        pretty_border()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dda817-6054-4ec7-9597-9d329f349d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
